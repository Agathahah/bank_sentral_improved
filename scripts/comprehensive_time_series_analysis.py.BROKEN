#!/usr/bin/env python3
"""
comprehensive_time_series_analysis.py - Time series analysis for central bank communication
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from scipy import signal
import warnings
warnings.filterwarnings('ignore')

class CommunicationTimeSeriesAnalyzer:
    """Comprehensive time series analysis for central bank communication effectiveness"""
    
    def __init__(self, df, date_col='Tanggal', text_col='Teks_Paragraf'):
        self.df = df.copy()
        self.date_col = date_col
        self.text_col = text_col
        self.prepare_temporal_data()
        
    def prepare_temporal_data(self):
        """Prepare data for time series analysis"""
        
        # Convert date column
        if self.date_col in self.df.columns:
            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col], errors='coerce')
            self.df = self.df.dropna(subset=[self.date_col])
            self.df = self.df.sort_values(self.date_col)
        else:
            # Create synthetic dates if no date column
            print("Warning: No date column found, creating synthetic temporal sequence")
            self.df['synthetic_date'] = pd.date_range(
                start='2019-01-01', 
                periods=len(self.df), 
                freq='W'
            )
            self.date_col = 'synthetic_date'
    
    def calculate_temporal_metrics(self):
        """Calculate time-varying communication metrics"""
        
        metrics_df = self.df.copy()
        
        # 1. CLARITY METRICS OVER TIME
        import textstat
        
        clarity_metrics = []
        for idx, row in metrics_df.iterrows():
            text = row[self.text_col] if pd.notna(row[self.text_col]) else ""
            
            if len(text.split()) >= 10:
                clarity_metrics.append({
                    'date': row[self.date_col],
                    'flesch_reading_ease': textstat.flesch_reading_ease(text),
                    'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),
                    'avg_sentence_length': len(text.split()) / max(len(text.split('.')), 1),
                    'complex_word_ratio': len([w for w in text.split() if len(w) > 7]) / len(text.split()),
                    'word_count': len(text.split())
                })
            else:
                clarity_metrics.append({
                    'date': row[self.date_col],
                    'flesch_reading_ease': np.nan,
                    'flesch_kincaid_grade': np.nan,
                    'avg_sentence_length': np.nan,
                    'complex_word_ratio': np.nan,
                    'word_count': 0
                })
        
        clarity_ts = pd.DataFrame(clarity_metrics).set_index('date')
        
        # 2. COMPREHENSIVENESS METRICS OVER TIME
        economic_terms = {
            'monetary_policy': ['inflasi', 'deflasi', 'moneter', 'kebijakan', 'transmisi'],
            'banking': ['bank', 'sentral', 'kredit', 'likuiditas', 'cadangan'],
            'interest_rates': ['suku', 'bunga', 'rate', 'repo', 'fasbi'],
            'exchange_rates': ['nilai', 'tukar', 'kurs', 'rupiah', 'valuta'],
            'economic_indicators': ['pertumbuhan', 'pdb', 'produksi', 'konsumsi'],
            'financial_markets': ['pasar', 'modal', 'saham', 'obligasi', 'yield']
        }
        
        comprehensiveness_metrics = []
        for idx, row in metrics_df.iterrows():
            text = row[self.text_col] if pd.notna(row[self.text_col]) else ""
            words = text.lower().split()
            
            total_coverage = 0
            category_metrics = {'date': row[self.date_col]}
            
            for category, terms in economic_terms.items():
                covered_terms = sum(1 for term in terms if term in words)
                coverage = covered_terms / len(terms)
                category_metrics[f'{category}_coverage'] = coverage
                total_coverage += covered_terms
            
            category_metrics.update({
                'total_economic_coverage': total_coverage / sum(len(terms) for terms in economic_terms.values()),
                'lexical_diversity': len(set(words)) / len(words) if words else 0,
                'unique_economic_terms': len(set(words) & set().union(*economic_terms.values()))
            })
            
            comprehensiveness_metrics.append(category_metrics)
        
        comprehensiveness_ts = pd.DataFrame(comprehensiveness_metrics).set_index('date')
        
        # 3. SENTIMENT METRICS OVER TIME
        sentiment_metrics = []
        sentiment_cols = [col for col in metrics_df.columns if 'sentimen' in col.lower() and 'majority' in col.lower()]
        
        if sentiment_cols:
            sentiment_col = sentiment_cols[0]
            sentiment_encoding = {'Positif': 1, 'Netral': 0, 'Negatif': -1}
            
            for idx, row in metrics_df.iterrows():
                sentiment = row[sentiment_col] if pd.notna(row[sentiment_col]) else "Netral"
                confidence_cols = [col for col in metrics_df.columns if 'confidence' in col.lower()]
                confidence = row[confidence_cols[0]] if confidence_cols and pd.notna(row[confidence_cols[0]]) else 0.5
                
                sentiment_metrics.append({
                    'date': row[self.date_col],
                    'sentiment_numeric': sentiment_encoding.get(sentiment, 0),
                    'sentiment_confidence': confidence,
                    'sentiment_strength': abs(sentiment_encoding.get(sentiment, 0)) * confidence,
                    'is_positive': 1 if sentiment == 'Positif' else 0,
                    'is_negative': 1 if sentiment == 'Negatif' else 0,
                    'is_neutral': 1 if sentiment == 'Netral' else 0
                })
        
        sentiment_ts = pd.DataFrame(sentiment_metrics).set_index('date') if sentiment_metrics else pd.DataFrame()
        
        return clarity_ts, comprehensiveness_ts, sentiment_ts
    
    def create_aggregated_time_series(self, clarity_ts, comprehensiveness_ts, sentiment_ts, freq='M'):
        """Create aggregated time series at specified frequency"""
        
        # Resample to specified frequency
        clarity_agg = clarity_ts.resample(freq).agg({
            'flesch_reading_ease': 'mean',
            'flesch_kincaid_grade': 'mean',
            'avg_sentence_length': 'mean',
            'complex_word_ratio': 'mean',
            'word_count': ['mean', 'sum', 'count']
        }).round(3)
        
        comprehensiveness_agg = comprehensiveness_ts.resample(freq).agg({
            'total_economic_coverage': 'mean',
            'lexical_diversity': 'mean',
            'unique_economic_terms': 'mean',
            'monetary_policy_coverage': 'mean',
            'banking_coverage': 'mean',
            'interest_rates_coverage': 'mean'
        }).round(3)
        
        if not sentiment_ts.empty:
            sentiment_agg = sentiment_ts.resample(freq).agg({
                'sentiment_numeric': 'mean',
                'sentiment_confidence': 'mean',
                'sentiment_strength': 'mean',
                'is_positive': 'sum',
                'is_negative': 'sum',
                'is_neutral': 'sum'
            }).round(3)
            
            # Calculate sentiment ratios
            total_docs = sentiment_agg[['is_positive', 'is_negative', 'is_neutral']].sum(axis=1)
            sentiment_agg['positive_ratio'] = sentiment_agg['is_positive'] / total_docs
            sentiment_agg['negative_ratio'] = sentiment_agg['is_negative'] / total_docs
            sentiment_agg['neutral_ratio'] = sentiment_agg['is_neutral'] / total_docs
        else:
            sentiment_agg = pd.DataFrame()
        
        return clarity_agg, comprehensiveness_agg, sentiment_agg
    
    def perform_trend_analysis(self, ts_data, metric_name):
        """Perform comprehensive trend analysis"""
        
        if ts_data.empty or ts_data.isna().all():
            return None
        
        # Fill missing values
        ts_clean = ts_data.interpolate().fillna(method='bfill').fillna(method='ffill')
        
        analysis_results = {
            'metric_name': metric_name,
            'data_points': len(ts_clean),
            'date_range': (ts_clean.index.min(), ts_clean.index.max())
        }
        
        # 1. Basic Statistics
        analysis_results['descriptive_stats'] = {
            'mean': ts_clean.mean(),
            'std': ts_clean.std(),
            'min': ts_clean.min(),
            'max': ts_clean.max(),
            'trend_direction': 'increasing' if ts_clean.iloc[-1] > ts_clean.iloc[0] else 'decreasing'
        }
        
        # 2. Stationarity Test
        try:
            adf_result = adfuller(ts_clean.dropna())
            analysis_results['stationarity'] = {
                'adf_statistic': adf_result[0],
                'p_value': adf_result[1],
                'is_stationary': adf_result[1] < 0.05
            }
        except:
            analysis_results['stationarity'] = {'is_stationary': None}
        
        # 3. Seasonal Decomposition (if enough data)
        if len(ts_clean) >= 24:  # Need at least 2 years of monthly data
            try:
                decomposition = seasonal_decompose(ts_clean, model='additive', period=12)
                analysis_results['seasonality'] = {
                    'seasonal_strength': np.std(decomposition.seasonal) / np.std(ts_clean),
                    'trend_strength': np.std(decomposition.trend.dropna()) / np.std(ts_clean),
                    'has_seasonality': np.std(decomposition.seasonal) / np.std(ts_clean) > 0.1
                }
            except:
                analysis_results['seasonality'] = {'has_seasonality': None}
        
        # 4. Change Point Detection
        if len(ts_clean) >= 10:
            # Simple change point detection using gradient
            gradient = np.gradient(ts_clean.values)
            change_points = signal.find_peaks(np.abs(gradient), height=np.std(gradient))[0]
            
            analysis_results['change_points'] = {
                'count': len(change_points),
                'dates': [ts_clean.index[i] for i in change_points] if len(change_points) > 0 else [],
                'magnitude': [gradient[i] for i in change_points] if len(change_points) > 0 else []
            }
        
        # 5. Volatility Analysis
        if len(ts_clean) >= 5:
            rolling_std = ts_clean.rolling(window=min(6, len(ts_clean)//2)).std()
            analysis_results['volatility'] = {
                'mean_volatility': rolling_std.mean(),
                'max_volatility': rolling_std.max(),
                'volatility_trend': 'increasing' if rolling_std.iloc[-1] > rolling_std.iloc[0] else 'decreasing'
            }
        
        return analysis_results
    
    def create_comprehensive_visualizations(self, clarity_agg, comprehensiveness_agg, sentiment_agg, output_dir):
        """Create comprehensive time series visualizations"""
        
        fig, axes = plt.subplots(4, 3, figsize=(20, 16))
        fig.suptitle('Central Bank Communication: Time Series Analysis', fontsize=16, fontweight='bold')
        
        # 1. CLARITY TRENDS
        if not clarity_agg.empty:
            # Reading Ease over time
            axes[0, 0].plot(clarity_agg.index, clarity_agg[('flesch_reading_ease', 'mean')], 
                           marker='o', linewidth=2, color='blue')
            axes[0, 0].set_title('Clarity: Reading Ease Over Time')
            axes[0, 0].set_ylabel('Flesch Reading Ease')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].axhline(y=60, color='green', linestyle='--', alpha=0.5, label='Easy Threshold')
            axes[0, 0].legend()
            
            # Word Count Trends
            axes[0, 1].plot(clarity_agg.index, clarity_agg[('word_count', 'mean')], 
                           marker='s', linewidth=2, color='purple')
            axes[0, 1].set_title('Communication Length Over Time')
            axes[0, 1].set_ylabel('Average Word Count')
            axes[0, 1].grid(True, alpha=0.3)
            
            # Complex Word Ratio
            axes[0, 2].plot(clarity_agg.index, clarity_agg[('complex_word_ratio', 'mean')], 
                           marker='^', linewidth=2, color='red')
            axes[0, 2].set_title('Complexity: Complex Word Ratio Over Time')
            axes[0, 2].set_ylabel('Complex Word Ratio')
            axes[0, 2].grid(True, alpha=0.3)
        
        # 2. COMPREHENSIVENESS TRENDS
        if not comprehensiveness_agg.empty:
            # Total Economic Coverage
            axes[1, 0].plot(comprehensiveness_agg.index, comprehensiveness_agg[('total_economic_coverage', 'mean')], 
                           marker='o', linewidth=2, color='green')
            axes[1, 0].set_title('Comprehensiveness: Economic Coverage Over Time')
            axes[1, 0].set_ylabel('Economic Term Coverage')
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='50% Threshold')
            axes[1, 0].legend()
            
            # Category Coverage Comparison
            category_cols = ['monetary_policy_coverage', 'banking_coverage', 'interest_rates_coverage']
            for i, col in enumerate(category_cols):
                if (col, 'mean') in comprehensiveness_agg.columns:
                    axes[1, 1].plot(comprehensiveness_agg.index, comprehensiveness_agg[(col, 'mean')], 
                                   marker='o', linewidth=2, label=col.replace('_coverage', '').replace('_', ' ').title())
            axes[1, 1].set_title('Coverage by Economic Category')
            axes[1, 1].set_ylabel('Coverage Ratio')
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].legend()
            
            # Lexical Diversity
            axes[1, 2].plot(comprehensiveness_agg.index, comprehensiveness_agg[('lexical_diversity', 'mean')], 
                           marker='s', linewidth=2, color='purple')
            axes[1, 2].set_title('Lexical Diversity Over Time')
            axes[1, 2].set_ylabel('Lexical Diversity Ratio')
            axes[1, 2].grid(True, alpha=0.3)
        
        # 3. SENTIMENT TRENDS
        if not sentiment_agg.empty:
            # Sentiment Score Over Time
            axes[2, 0].plot(sentiment_agg.index, sentiment_agg[('sentiment_numeric', 'mean')], 
                           marker='o', linewidth=2, color='blue')
            axes[2, 0].set_title('Sentiment Score Over Time')
            axes[2, 0].set_ylabel('Average Sentiment (-1 to 1)')
            axes[2, 0].grid(True, alpha=0.3)
            axes[2, 0].axhline(y=0, color='gray', linestyle='-', alpha=0.5)
            axes[2, 0].axhline(y=0.2, color='green', linestyle='--', alpha=0.5, label='Positive Threshold')
            axes[2, 0].axhline(y=-0.2, color='red', linestyle='--', alpha=0.5, label='Negative Threshold')
            axes[2, 0].legend()
            
            # Sentiment Distribution Over Time (Stacked)
            axes[2, 1].stackplot(sentiment_agg.index, 
                               sentiment_agg['positive_ratio'], 
                               sentiment_agg['neutral_ratio'], 
                               sentiment_agg['negative_ratio'],
                               labels=['Positive', 'Neutral', 'Negative'],
                               colors=['green', 'gray', 'red'], alpha=0.7)
            axes[2, 1].set_title('Sentiment Distribution Over Time')
            axes[2, 1].set_ylabel('Proportion')
            axes[2, 1].legend(loc='upper right')
            axes[2, 1].grid(True, alpha=0.3)
            
            # Sentiment Confidence Over Time
            axes[2, 2].plot(sentiment_agg.index, sentiment_agg[('sentiment_confidence', 'mean')], 
                           marker='^', linewidth=2, color='orange')
            axes[2, 2].set_title('Sentiment Confidence Over Time')
            axes[2, 2].set_ylabel('Average Confidence')
            axes[2, 2].grid(True, alpha=0.3)
            axes[2, 2].axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='High Confidence')
            axes[2, 2].legend()
        
        # 4. INTEGRATED ANALYSIS
        # Communication Effectiveness Index (combine all metrics)
        if not clarity_agg.empty and not comprehensiveness_agg.empty:
            # Normalize metrics to 0-1 scale
            clarity_norm = (clarity_agg[('flesch_reading_ease', 'mean')] - 
                           clarity_agg[('flesch_reading_ease', 'mean')].min()) / \
                          (clarity_agg[('flesch_reading_ease', 'mean')].max() - 
                           clarity_agg[('flesch_reading_ease', 'mean')].min())
            
            comprehensiveness_norm = comprehensiveness_agg[('total_economic_coverage', 'mean')]
            
            if not sentiment_agg.empty:
                sentiment_norm = (sentiment_agg[('sentiment_numeric', 'mean')] + 1) / 2  # Scale to 0-1
                effectiveness_index = (clarity_norm + comprehensiveness_norm + sentiment_norm) / 3
            else:
                effectiveness_index = (clarity_norm + comprehensiveness_norm) / 2
            
            axes[3, 0].plot(effectiveness_index.index, effectiveness_index.values, 
                           marker='o', linewidth=3, color='darkblue')
            axes[3, 0].fill_between(effectiveness_index.index, effectiveness_index.values, 
                                   alpha=0.3, color='lightblue')
            axes[3, 0].set_title('Communication Effectiveness Index')
            axes[3, 0].set_ylabel('Effectiveness Score (0-1)')
            axes[3, 0].grid(True, alpha=0.3)
            axes[3, 0].axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Target Level')
            axes[3, 0].legend()
        
        # Communication Volume Over Time
        if not clarity_agg.empty:
            axes[3, 1].bar(clarity_agg.index, clarity_agg[('word_count', 'count')], 
                          color='lightcoral', alpha=0.7)
            axes[3, 1].set_title('Communication Volume (Number of Documents)')
            axes[3, 1].set_ylabel('Document Count')
            axes[3, 1].grid(True, alpha=0.3)
        
        # Volatility Analysis (Rolling Standard Deviation)
        if not clarity_agg.empty and len(clarity_agg) >= 6:
            rolling_clarity = clarity_agg[('flesch_reading_ease', 'mean')].rolling(window=6).std()
            rolling_comprehensiveness = comprehensiveness_agg[('total_economic_coverage', 'mean')].rolling(window=6).std()
            
            axes[3, 2].plot(rolling_clarity.index, rolling_clarity.values, 
                           marker='o', linewidth=2, label='Clarity Volatility', color='blue')
            axes[3, 2].plot(rolling_comprehensiveness.index, rolling_comprehensiveness.values, 
                           marker='s', linewidth=2, label='Comprehensiveness Volatility', color='green')
            axes[3, 2].set_title('Communication Consistency (6-Month Rolling Volatility)')
            axes[3, 2].set_ylabel('Standard Deviation')
            axes[3, 2].grid(True, alpha=0.3)
            axes[3, 2].legend()
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/comprehensive_time_series_analysis.png', 
                    dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_time_series_insights(self, clarity_analysis, comprehensiveness_analysis, 
                                     sentiment_analysis, output_dir):
        """Generate comprehensive insights report"""
        
        insights_report = f"""
COMPREHENSIVE TIME SERIES ANALYSIS REPORT
=========================================
Central Bank Communication Effectiveness Over Time

EXECUTIVE SUMMARY
================

This report provides temporal analysis of central bank communication across three key dimensions:
1. CLARITY - How easily the communication can be understood
2. COMPREHENSIVENESS - How thoroughly economic topics are covered  
3. SENTIMENT - The emotional tone and confidence of communication

"""
        
        # CLARITY INSIGHTS
        if clarity_analysis:
            insights_report += f"""
1. CLARITY ANALYSIS
==================

Temporal Characteristics:
- Data Points: {clarity_analysis['data_points']}
- Analysis Period: {clarity_analysis['date_range'][0].strftime('%Y-%m')} to {clarity_analysis['date_range'][1].strftime('%Y-%m')}
- Trend Direction: {clarity_analysis['descriptive_stats']['trend_direction']}

Key Metrics:
- Average Reading Ease: {clarity_analysis['descriptive_stats']['mean']:.2f}
- Volatility (Std Dev): {clarity_analysis['descriptive_stats']['std']:.2f}
- Range: {clarity_analysis['descriptive_stats']['min']:.2f} - {clarity_analysis['descriptive_stats']['max']:.2f}

Statistical Properties:
- Stationarity: {"Yes" if clarity_analysis.get('stationarity', {}).get('is_stationary') else "No"}
- Seasonality: {"Detected" if clarity_analysis.get('seasonality', {}).get('has_seasonality') else "Not Detected"}
- Change Points: {clarity_analysis.get('change_points', {}).get('count', 0)} significant changes detected

CLARITY INTERPRETATION:
"""
            
            avg_clarity = clarity_analysis['descriptive_stats']['mean']
            if avg_clarity >= 80:
                clarity_level = "EXCELLENT - Very easy to understand"
            elif avg_clarity >= 60:
                clarity_level = "GOOD - Easily understood by most readers"
            elif avg_clarity >= 30:
                clarity_level = "MODERATE - Requires secondary education level"
            else:
                clarity_level = "DIFFICULT - Requires higher education level"
            
            insights_report += f"- Overall Clarity Level: {clarity_level}\n"
            
            if clarity_analysis['descriptive_stats']['trend_direction'] == 'increasing':
                insights_report += "- POSITIVE TREND: Communication clarity is improving over time\n"
            else:
                insights_report += "- CONCERN: Communication clarity is declining over time\n"
        
        # COMPREHENSIVENESS INSIGHTS
        if comprehensiveness_analysis:
            insights_report += f"""

2. COMPREHENSIVENESS ANALYSIS
============================

Temporal Characteristics:
- Data Points: {comprehensiveness_analysis['data_points']}
- Trend Direction: {comprehensiveness_analysis['descriptive_stats']['trend_direction']}

Key Metrics:
- Average Economic Coverage: {comprehensiveness_analysis['descriptive_stats']['mean']:.2%}
- Coverage Volatility: {comprehensiveness_analysis['descriptive_stats']['std']:.3f}
- Coverage Range: {comprehensiveness_analysis['descriptive_stats']['min']:.2%} - {comprehensiveness_analysis['descriptive_stats']['max']:.2%}

Statistical Properties:
- Stationarity: {"Yes" if comprehensiveness_analysis.get('stationarity', {}).get('is_stationary') else "No"}
- Change Points: {comprehensiveness_analysis.get('change_points', {}).get('count', 0)} significant changes

COMPREHENSIVENESS INTERPRETATION:
"""
            
            avg_coverage = comprehensiveness_analysis['descriptive_stats']['mean']
            if avg_coverage >= 0.7:
                coverage_level = "EXCELLENT - Comprehensive economic coverage"
            elif avg_coverage >= 0.5:
                coverage_level = "GOOD - Adequate economic coverage"
            elif avg_coverage >= 0.3:
                coverage_level = "MODERATE - Partial economic coverage"
            else:
                coverage_level = "INSUFFICIENT - Limited economic coverage"
            
            insights_report += f"- Overall Coverage Level: {coverage_level}\n"
        
        # SENTIMENT INSIGHTS
        if sentiment_analysis:
            insights_report += f"""

3. SENTIMENT ANALYSIS
====================

Temporal Characteristics:
- Data Points: {sentiment_analysis['data_points']}
- Trend Direction: {sentiment_analysis['descriptive_stats']['trend_direction']}

Key Metrics:
- Average Sentiment: {sentiment_analysis['descriptive_stats']['mean']:.3f} (-1 to 1 scale)
- Sentiment Volatility: {sentiment_analysis['descriptive_stats']['std']:.3f}
- Sentiment Range: {sentiment_analysis['descriptive_stats']['min']:.3f} - {sentiment_analysis['descriptive_stats']['max']:.3f}

Statistical Properties:
- Stationarity: {"Yes" if sentiment_analysis.get('stationarity', {}).get('is_stationary') else "No"}
- Change Points: {sentiment_analysis.get('change_points', {}).get('count', 0)} significant sentiment shifts

SENTIMENT INTERPRETATION:
"""
            
            avg_sentiment = sentiment_analysis['descriptive_stats']['mean']
            if avg_sentiment >= 0.3:
                sentiment_level = "POSITIVE - Optimistic communication tone"
            elif avg_sentiment >= -0.3:
                sentiment_level = "NEUTRAL - Balanced communication tone"
            else:
                sentiment_level = "NEGATIVE - Cautious/Warning communication tone"
            
            insights_report += f"- Overall Sentiment: {sentiment_level}\n"
        
        # STRATEGIC RECOMMENDATIONS
        insights_report += f"""

4. STRATEGIC RECOMMENDATIONS
============================

Based on the temporal analysis, here are key recommendations:

IMMEDIATE ACTIONS (0-3 months):
"""
        
        recommendations = []
        
        if clarity_analysis and clarity_analysis['descriptive_stats']['trend_direction'] == 'decreasing':
            recommendations.append("- PRIORITY: Implement clarity improvement program")
            recommendations.append("- Review and simplify technical language usage")
        
        if comprehensiveness_analysis and comprehensiveness_analysis['descriptive_stats']['mean'] < 0.5:
            recommendations.append("- PRIORITY: Enhance economic terminology coverage")
            recommendations.append("- Develop comprehensive communication templates")
        
        if sentiment_analysis and sentiment_analysis.get('volatility', {}).get('mean_volatility', 0) > 0.3:
            recommendations.append("- PRIORITY: Stabilize communication tone consistency")
            recommendations.append("- Implement sentiment guidelines for crisis communication")
        
        if not recommendations:
            recommendations.append("- MAINTAIN: Current communication quality appears adequate")
            recommendations.append("- MONITOR: Continue tracking temporal trends")
        
        for rec in recommendations:
            insights_report += f"{rec}\n"
        
        insights_report += f"""

MEDIUM-TERM STRATEGIES (3-12 months):
- Develop predictive models for communication impact
- Implement real-time monitoring dashboards
- Create adaptive communication strategies based on temporal patterns
- Establish communication effectiveness benchmarks

LONG-TERM INITIATIVES (1-2 years):
- Build AI-powered communication optimization system
- Develop cross-temporal consistency frameworks
- Create scenario-based communication strategies
- Implement continuous improvement feedback loops

5. METHODOLOGICAL NOTES
=======================

Analysis Techniques Used:
- Augmented Dickey-Fuller test for stationarity
- Seasonal decomposition (when sufficient data)
- Change point detection using gradient analysis
- Rolling volatility analysis
- Trend analysis using linear regression

Data Quality Considerations:
- Missing data handled through interpolation
- Outliers identified and analyzed separately
- Temporal aggregation performed monthly for trend analysis
- Statistical significance tested at 95% confidence level

Limitations:
- Analysis dependent on data quality and temporal coverage
- External factors (economic events) not explicitly modeled
- Causal relationships require additional econometric analysis
- Results specific to Indonesian central bank communication context

6. CONCLUSION
============

The temporal analysis reveals important patterns in central bank communication effectiveness.
Key findings suggest that systematic monitoring and optimization of communication features
can significantly enhance policy transmission and public understanding.

Regular temporal analysis should be integrated into communication strategy development
to ensure continuous improvement and adaptation to changing economic conditions.

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Analysis Period: Full dataset temporal range
"""
        
        # Save insights report
        with open(f'{output_dir}/time_series_insights_report.txt', 'w', encoding='utf-8') as f:
            f.write(insights_report)
        
        return insights_report

# Usage Example and Integration
def integrate_time_series_with_existing_pipeline(merged_file, output_dir):
    """Integration function for existing pipeline"""
    
    # Load data
    df = pd.read_excel(merged_file)
    
    # Initialize analyzer
    analyzer = CommunicationTimeSeriesAnalyzer(df)
    
    # Calculate temporal metrics
    clarity_ts, comprehensiveness_ts, sentiment_ts = analyzer.calculate_temporal_metrics()
    
    # Create aggregated time series
    clarity_agg, comprehensiveness_agg, sentiment_agg = analyzer.create_aggregated_time_series(
        clarity_ts, comprehensiveness_ts, sentiment_ts, freq='M'
    )
    
    # Perform trend analysis
    clarity_analysis = analyzer.perform_trend_analysis(
        clarity_agg[('flesch_reading_ease', 'mean')] if not clarity_agg.empty else pd.Series(), 
        'Clarity'
    )
    
    comprehensiveness_analysis = analyzer.perform_trend_analysis(
        comprehensiveness_agg[('total_economic_coverage', 'mean')] if not comprehensiveness_agg.empty else pd.Series(),
        'Comprehensiveness'
    )
    
    sentiment_analysis = analyzer.perform_trend_analysis(
        sentiment_agg[('sentiment_numeric', 'mean')] if not sentiment_agg.empty else pd.Series(),
        'Sentiment'
    )
    
    # Create visualizations
    analyzer.create_comprehensive_visualizations(
        clarity_agg, comprehensiveness_agg, sentiment_agg, output_dir
    )
    
    # Generate insights
    insights = analyzer.generate_time_series_insights(
        clarity_analysis, comprehensiveness_analysis, sentiment_analysis, output_dir
    )
    
    print(f"Time series analysis completed. Results saved to: {output_dir}")
    return analyzer, insights

if __name__ == "__main__":
    # Example usage
    merged_file = "path/to/merged_annotations.xlsx"
    output_dir = "output/time_series_analysis"
    
    analyzer, insights = integrate_time_series_with_existing_pipeline(merged_file, output_dir)